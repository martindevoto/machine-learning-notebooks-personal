{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFbqXxq6FOt2IoMwhAnUsH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martindevoto/machine-learning-notebooks-personal/blob/main/Intro_Haystack_pt_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Your Own \"Dense Passage Retrieval\" Model\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial9_DPR_training.ipynb)\n",
        "\n",
        "Haystack contains all the tools needed to train your own Dense Passage Retrieval model.\n",
        "This tutorial will guide you through the steps required to create a retriever that is specifically tailored to your domain."
      ],
      "metadata": {
        "id": "rT9Yf_12pFBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sagFEpUo_6w",
        "outputId": "752fb26a-7d24-488b-efb2-4d7c7c3e63f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.0.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.0.3\n",
            "Collecting farm-haystack[colab]\n",
            "  Cloning https://github.com/deepset-ai/haystack.git to /tmp/pip-install-1_po_4fw/farm-haystack_099f4ebda9a4467b87293596da657e71\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git /tmp/pip-install-1_po_4fw/farm-haystack_099f4ebda9a4467b87293596da657e71\n",
            "  Resolved https://github.com/deepset-ai/haystack.git to commit 40328a57b6d5760e51eeb9366838ce2b25f5f2fa\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (4.62.3)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (3.2.5)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (2.6.3)\n",
            "Collecting tika\n",
            "  Downloading tika-1.24.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting quantulum3\n",
            "  Downloading quantulum3-0.7.9-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (2.23.0)\n",
            "Collecting mmh3\n",
            "  Downloading mmh3-3.0.0-cp37-cp37m-manylinux2010_x86_64.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlflow<=1.13.1\n",
            "  Downloading mlflow-1.13.1-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (4.10.1)\n",
            "Collecting elasticsearch<=7.10,>=7.7\n",
            "  Downloading elasticsearch-7.10.0-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 KB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elastic-apm\n",
            "  Downloading elastic_apm-6.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (359 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.7/359.7 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers>=0.4.0\n",
            "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.3.5)\n",
            "Collecting transformers==4.13.0\n",
            "  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<1.11,>1.9 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.10.0+cu111)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (8.12.0)\n",
            "Collecting azure-ai-formrecognizer==3.2.0b2\n",
            "  Downloading azure_ai_formrecognizer-3.2.0b2-py2.py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.7/219.7 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio==1.43.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.43.0)\n",
            "Collecting azure-core<2.0.0,>=1.13.0\n",
            "  Downloading azure_core-1.22.0-py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.6/178.6 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-common~=1.1\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack[colab]) (1.15.0)\n",
            "Collecting msrest>=0.6.21\n",
            "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack[colab]) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack[colab]) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack[colab]) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack[colab]) (3.4.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 KB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 KB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack[colab]) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack[colab]) (2021.10.8)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.18.7-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab]) (0.4.2)\n",
            "Collecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting azure-storage-blob>=12.0.0\n",
            "  Downloading azure_storage_blob-12.9.0-py2.py3-none-any.whl (356 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.5/356.5 KB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker>=4.0.0\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.2/146.2 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab]) (1.3.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab]) (0.4)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.16.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab]) (2.8.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab]) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab]) (3.17.3)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.9/180.9 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab]) (1.1.4)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab]) (1.4.31)\n",
            "Collecting alembic<=1.4.1\n",
            "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[colab]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[colab]) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (3.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.0->farm-haystack[colab]) (0.11.1+cu111)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.11,>1.9->farm-haystack[colab]) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->farm-haystack[colab]) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->farm-haystack[colab]) (2018.9)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx->farm-haystack[colab]) (4.2.6)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from quantulum3->farm-haystack[colab]) (2.1.0)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika->farm-haystack[colab]) (57.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting cryptography>=2.1.4\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm-haystack[colab]) (0.8.9)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b2->farm-haystack[colab]) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0->farm-haystack[colab]) (3.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow<=1.13.1->farm-haystack[colab]) (1.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow<=1.13.1->farm-haystack[colab]) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow<=1.13.1->farm-haystack[colab]) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow<=1.13.1->farm-haystack[colab]) (2.11.3)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words->quantulum3->farm-haystack[colab]) (0.6.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow<=1.13.1->farm-haystack[colab]) (0.13.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.4.0->farm-haystack[colab]) (7.1.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm-haystack[colab]) (1.15.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow<=1.13.1->farm-haystack[colab]) (2.0.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b2->farm-haystack[colab]) (3.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm-haystack[colab]) (2.21)\n",
            "Building wheels for collected packages: sentence-transformers, farm-haystack, langdetect, python-docx, seqeval, tika, alembic, databricks-cli\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=120999 sha256=960f809d0ddb052a2a3462bc1f8d8780ae659b73f0a74beb8bc33f6bdb433a59\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n",
            "  Building wheel for farm-haystack (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm-haystack: filename=farm_haystack-1.1.0-py3-none-any.whl size=405744 sha256=7ac9d5ea8ce76b713dde12e1835729ff7677991d21510eb0fc3ede2b31588311\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0r1w0swl/wheels/a7/05/3b/9b33368d9af06a39f8e6af2e97fa2af876e893ade323cfc2c9\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=23195081a49c076b6546a77d95cfe538df1f0ae8a07083ad4d2e67294d11f6d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=16535dea8859783e789af078bc75217c4b53f78109e830573d2311e95fe72890\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=0869fc66e9a8b40a7686b6889c9137096589dab58930efbf50b254ca841a57b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32893 sha256=e7918549699c1bb2829d0090ca7ae648129cc714de8781c6235a6de87ae6684f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158171 sha256=303f6c19688ab4bffa09e0535936bd564f914947f43637c79500ae0e83daff09\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.16.4-py3-none-any.whl size=106877 sha256=c62369edc5884f459f9e5d40a6fe755d00779d012186e746ff524853c1b8144e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/a1/6d/fa1d22ea25ed8593887437fe1c7e00f6ef307fc240ccd4dc5c\n",
            "Successfully built sentence-transformers farm-haystack langdetect python-docx seqeval tika alembic databricks-cli\n",
            "Installing collected packages: tokenizers, sentencepiece, python-editor, mmh3, azure-common, websocket-client, smmap, sacremoses, querystring-parser, pyyaml, python-docx, pydantic, num2words, Mako, langdetect, isodate, gunicorn, elasticsearch, elastic-apm, tika, quantulum3, huggingface-hub, gitdb, docker, databricks-cli, cryptography, azure-core, transformers, seqeval, prometheus-flask-exporter, msrest, gitpython, alembic, sentence-transformers, azure-storage-blob, azure-ai-formrecognizer, mlflow, farm-haystack\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed Mako-1.1.6 alembic-1.4.1 azure-ai-formrecognizer-3.2.0b2 azure-common-1.1.28 azure-core-1.22.0 azure-storage-blob-12.9.0 cryptography-36.0.1 databricks-cli-0.16.4 docker-5.0.3 elastic-apm-6.7.2 elasticsearch-7.10.0 farm-haystack-1.1.0 gitdb-4.0.9 gitpython-3.1.26 gunicorn-20.1.0 huggingface-hub-0.4.0 isodate-0.6.1 langdetect-1.0.9 mlflow-1.13.1 mmh3-3.0.0 msrest-0.6.21 num2words-0.5.10 prometheus-flask-exporter-0.18.7 pydantic-1.9.0 python-docx-0.8.11 python-editor-1.0.4 pyyaml-6.0 quantulum3-0.7.9 querystring-parser-1.2.4 sacremoses-0.0.47 sentence-transformers-2.1.0 sentencepiece-0.1.96 seqeval-1.2.2 smmap-5.0.0 tika-1.24 tokenizers-0.10.3 transformers-4.13.0 websocket-client-1.2.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install the latest release of Haystack in your own environment\n",
        "#! pip install farm-haystack\n",
        "\n",
        "# Install the latest master of Haystack\n",
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are some import that we'll need\n",
        "\n",
        "from haystack.nodes import DensePassageRetriever\n",
        "from haystack.utils import fetch_archive_from_http\n",
        "from haystack.document_stores import InMemoryDocumentStore"
      ],
      "metadata": {
        "id": "WQKNWKvEpHDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Data\n",
        "\n",
        "DPR training performed using Information Retrieval data.\n",
        "More specifically, you want to feed in pairs of queries and relevant documents.\n",
        "\n",
        "To train a model, we will need a dataset that has the same format as the original DPR training data.\n",
        "Each data point in the dataset should have the following dictionary structure.\n",
        "\n",
        "``` python\n",
        "    {\n",
        "        \"dataset\": str,\n",
        "        \"question\": str,\n",
        "        \"answers\": list of str\n",
        "        \"positive_ctxs\": list of dictionaries of format {'title': str, 'text': str, 'score': int, 'title_score': int, 'passage_id': str}\n",
        "        \"negative_ctxs\": list of dictionaries of format {'title': str, 'text': str, 'score': int, 'title_score': int, 'passage_id': str}\n",
        "        \"hard_negative_ctxs\": list of dictionaries of format {'title': str, 'text': str, 'score': int, 'title_score': int, 'passage_id': str}\n",
        "    }\n",
        "```\n",
        "\n",
        "`positive_ctxs` are context passages which are relevant to the query.\n",
        "In some datasets, queries might have more than one positive context\n",
        "in which case you can set the `num_positives` parameter to be higher than the default 1.\n",
        "Note that `num_positives` needs to be lower or equal to the minimum number of `positive_ctxs` for queries in your data.\n",
        "If you have an unequal number of positive contexts per example,\n",
        "you might want to generate some soft labels by retrieving similar contexts which contain the answer.\n",
        "\n",
        "DPR is standardly trained using a method known as in-batch negatives.\n",
        "This means that positive contexts for a given query are treated as negative contexts for the other queries in the batch.\n",
        "Doing so allows for a high degree of computational efficiency, thus allowing the model to be trained on large amounts of data.\n",
        "\n",
        "`negative_ctxs` is not actually used in Haystack's DPR training so we recommend you set it to an empty list.\n",
        "They were used by the original DPR authors in an experiment to compare it against the in-batch negatives method.\n",
        "\n",
        "`hard_negative_ctxs` are passages that are not relevant to the query.\n",
        "In the original DPR paper, these are fetched using a retriever to find the most relevant passages to the query.\n",
        "Passages which contain the answer text are filtered out.\n",
        "\n",
        "If you'd like to convert your SQuAD format data into something that can train a DPR model,\n",
        "check out the utility script at [`haystack/retriever/squad_to_dpr.py`](https://github.com/deepset-ai/haystack/blob/master/haystack/retriever/squad_to_dpr.py)"
      ],
      "metadata": {
        "id": "PmsGC8lApXMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Question Answering Data\n",
        "\n",
        "Question Answering datasets can sometimes be used as training data.\n",
        "Google's Natural Questions dataset, is sufficiently large\n",
        "and contains enough unique passages, that it can be converted into a DPR training set.\n",
        "This is done simply by considering answer containing passages as relevant documents to the query.\n",
        "\n",
        "The SQuAD dataset, however, is not as suited to this use case since its question and answer pairs\n",
        "are created on only a very small slice of wikipedia documents."
      ],
      "metadata": {
        "id": "eA4oM2qU8OCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Original DPR Training Data\n",
        "\n",
        "WARNING: These files are large! The train set is 7.4GB and the dev set is 800MB\n",
        "\n",
        "We can download the original DPR training data with the following cell.\n",
        "Note that this data is probably only useful if you are trying to train from scratch."
      ],
      "metadata": {
        "id": "xfp8FWV88Pdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download original DPR data\n",
        "# WARNING: the train set is 7.4GB and the dev set is 800MB\n",
        "\n",
        "doc_dir = 'data/dpr_training/'\n",
        "\n",
        "s3_url_train = \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz\"\n",
        "s3_url_dev = \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\"\n",
        "\n",
        "fetch_archive_from_http(s3_url_train, output_dir=doc_dir + 'train/')\n",
        "fetch_archive_from_http(s3_url_dev, output_dir=doc_dir + 'dev/')"
      ],
      "metadata": {
        "id": "8kgFHU0ZpUnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1: Training DPR from Scratch\n",
        "\n",
        "The default variables that we provide below are chosen to train a DPR model from scratch.\n",
        "Here, both passage and query embedding models are initialized using BERT base\n",
        "and the model is trained using Google's Natural Questions dataset (in a format specialised for DPR).\n",
        "\n",
        "If you are working in a language other than English,\n",
        "you will want to initialize the passage and query embedding models with a language model that supports your language\n",
        "and also provide a dataset in your language."
      ],
      "metadata": {
        "id": "ZZGFHEX394Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are the variables to specify our training data, the models that we use to\n",
        "# initialize DPR and the directory where we'll be saving the model\n",
        "\n",
        "doc_dir = 'data/dpr_training'\n",
        "\n",
        "train_filename = 'train/biencoder-nq-train.json'\n",
        "dev_filename = 'dev/biencoder-nq-dev.json'\n",
        "\n",
        "query_model = 'bert-base-uncased'\n",
        "passage_model = 'bert-base-uncased'\n",
        "\n",
        "save_dir = '../saved_models/dpr'"
      ],
      "metadata": {
        "id": "anWeZRRv94N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Finetuning DPR\n",
        "\n",
        "If you have your own domain specific question answering or information retrieval dataset,\n",
        "you might instead be interested in finetuning a pretrained DPR model.\n",
        "In this case, you would initialize both query and passage models using the original pretrained model.\n",
        "You will want to load something like this set of variables instead of the ones above"
      ],
      "metadata": {
        "id": "6Gv_EU1H-9hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are the variables you might want to use instead of the set above\n",
        "# in order to perform pretraining\n",
        "\n",
        "doc_dir = \"PATH_TO_YOU_DATA_DIR\"\n",
        "train_filename = 'TRAIN_FILENAME'\n",
        "dev_filename = 'DEV_FILENAME'\n",
        "\n",
        "query_model = 'facebook/dpr-question_encoder-single-nq-base'\n",
        "passage_model = 'facebook/dpr-ctx_encoder-single-nq-base'\n",
        "\n",
        "save_dir = '../saved_models/dpr'"
      ],
      "metadata": {
        "id": "VRylxek5-9p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization\n",
        "\n",
        "Here we want to initialize our model either with plain language model weights for training from scratch\n",
        "or else with pretrained DPR weights for finetuning.\n",
        "We follow the [original DPR parameters](https://github.com/facebookresearch/DPR#best-hyperparameter-settings)\n",
        "for their max passage length but set max query length to 64 since queries are very rarely longer."
      ],
      "metadata": {
        "id": "wmPUPemd_ZiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize DPR model\n",
        "\n",
        "retriever = DensePassageRetriever(\n",
        "    document_store=InMemoryDocumentStore(),\n",
        "    query_embedding_model=query_model,\n",
        "    passage_embedding_model=passage_model,\n",
        "    max_seq_len_query=64,\n",
        "    max_seq_len_passage=256\n",
        ")"
      ],
      "metadata": {
        "id": "8i4e_ifn_asz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Let's start training and save our trained model!\n",
        "\n",
        "On a V100 GPU, you can fit up to batch size 16 so we set gradient accumulation steps to 8 in order\n",
        "to simulate the batch size 128 of the original DPR experiment.\n",
        "\n",
        "When `embed_title=True`, the document title is prepended to the input text sequence with a `[SEP]` token\n",
        "between it and document text."
      ],
      "metadata": {
        "id": "QN8EO2_XAWAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training from scratch with the above variables, 1 epoch takes around an hour and we reached the following performance:\n",
        "\n",
        "```\n",
        "loss: 0.046580662854042276\n",
        "task_name: text_similarity\n",
        "acc: 0.992524064068483\n",
        "f1: 0.8804297774366846\n",
        "acc_and_f1: 0.9364769207525838\n",
        "average_rank: 0.19631619339984652\n",
        "report:\n",
        "                precision    recall  f1-score   support\n",
        "\n",
        "hard_negative     0.9961    0.9961    0.9961    201887\n",
        "     positive     0.8804    0.8804    0.8804      6515\n",
        "\n",
        "     accuracy                         0.9925    208402\n",
        "    macro avg     0.9383    0.9383    0.9383    208402\n",
        " weighted avg     0.9925    0.9925    0.9925    208402\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "XR0d-BvsAak9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training our model and save it when it is finished\n",
        "\n",
        "retriever.train(\n",
        "    data_dir=doc_dir,\n",
        "    train_filename=train_filename,\n",
        "    dev_filename=dev_filename,\n",
        "    test_filename=test_filename,\n",
        "    n_epochs=1,\n",
        "    batch_size=16,\n",
        "    grad_acc_steps=8,\n",
        "    save_dir=save_dir,\n",
        "    evaluate_every=3000,\n",
        "    embed_title=True,\n",
        "    num_positives=1,\n",
        "    num_hard_negatives=1,\n",
        ")"
      ],
      "metadata": {
        "id": "1SmMdWGjAWJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading\n",
        "\n",
        "Loading our newly trained model is simple!"
      ],
      "metadata": {
        "id": "dyCh2WhgAxgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_retriever = DensePassageRetriever.load(load_dir=save_dir, \n",
        "                                                document_store=None)"
      ],
      "metadata": {
        "id": "n3I-o0oZAyTZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}