{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO38EZL+DwrloIcObTfO31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martindevoto/machine-learning-notebooks-personal/blob/main/Intro_Haystack_pt_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial8_Preprocessing.ipynb)\n",
        "\n",
        "Haystack includes a suite of tools to extract text from different file types, normalize white space\n",
        "and split text into smaller pieces to optimize retrieval.\n",
        "These data preprocessing steps can have a big impact on the systems performance and effective handling of data is key to getting the most out of Haystack."
      ],
      "metadata": {
        "id": "hpW0Fjkvn_nP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultimately, Haystack expects data to be provided as a list documents in the following dictionary format:\n",
        "``` python\n",
        "docs = [\n",
        "    {\n",
        "        'content': DOCUMENT_TEXT_HERE,\n",
        "        'meta': {'name': DOCUMENT_NAME, ...}\n",
        "    }, ...\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "qhWIuHdzoOIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial will show you all the tools that Haystack provides to help you cast your data into this format."
      ],
      "metadata": {
        "id": "G_URvnR1oSPs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA2juUN4n6uC",
        "outputId": "fc888cae-2ed8-4141-d308-c7f462d96e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.0.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.0.3\n",
            "Collecting farm-haystack[colab,ocr]\n",
            "  Cloning https://github.com/deepset-ai/haystack.git to /tmp/pip-install-okyfk4nk/farm-haystack_6e18cba158fb476ca87145ab2bac34b9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git /tmp/pip-install-okyfk4nk/farm-haystack_6e18cba158fb476ca87145ab2bac34b9\n",
            "  Resolved https://github.com/deepset-ai/haystack.git to commit aca52ea39c4923a9a1c03172c1d22da7f60e505f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (2.6.3)\n",
            "Collecting elastic-apm\n",
            "  Downloading elastic_apm-6.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (359 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.7/359.7 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (4.10.1)\n",
            "Collecting mmh3\n",
            "  Downloading mmh3-3.0.0-cp37-cp37m-manylinux2010_x86_64.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (8.12.0)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (0.3.4)\n",
            "Collecting mlflow<=1.13.1\n",
            "  Downloading mlflow-1.13.1-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (3.2.5)\n",
            "Collecting sentence-transformers>=0.4.0\n",
            "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (1.4.1)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting azure-ai-formrecognizer==3.2.0b2\n",
            "  Downloading azure_ai_formrecognizer-3.2.0b2-py2.py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.7/219.7 KB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<1.11,>1.9 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (1.0.2)\n",
            "Collecting transformers==4.13.0\n",
            "  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (2.23.0)\n",
            "Collecting elasticsearch<=7.10,>=7.7\n",
            "  Downloading elasticsearch-7.10.0-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 KB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting quantulum3\n",
            "  Downloading quantulum3-0.7.9-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tika\n",
            "  Downloading tika-1.24.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (7.1.2)\n",
            "Collecting pytesseract==0.3.7\n",
            "  Downloading pytesseract-0.3.7.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdf2image==1.14.0\n",
            "  Downloading pdf2image-1.14.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: grpcio==1.43.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab,ocr]) (1.43.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack[colab,ocr]) (1.15.0)\n",
            "Collecting azure-common~=1.1\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Collecting msrest>=0.6.21\n",
            "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.13.0\n",
            "  Downloading azure_core-1.22.0-py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.6/178.6 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack[colab,ocr]) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack[colab,ocr]) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 KB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 KB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack[colab,ocr]) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack[colab,ocr]) (3.4.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack[colab,ocr]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack[colab,ocr]) (1.24.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab,ocr]) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab,ocr]) (2.8.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab,ocr]) (0.4)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic<=1.4.1\n",
            "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab,ocr]) (0.4.2)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab,ocr]) (1.4.31)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab,ocr]) (7.1.2)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab,ocr]) (1.1.4)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.18.7-py3-none-any.whl (17 kB)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.9/180.9 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker>=4.0.0\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.2/146.2 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.16.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack[colab,ocr]) (3.17.3)\n",
            "Collecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting azure-storage-blob>=12.0.0\n",
            "  Downloading azure_storage_blob-12.9.0-py2.py3-none-any.whl (356 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.5/356.5 KB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[colab,ocr]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[colab,ocr]) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab,ocr]) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab,ocr]) (3.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.0->farm-haystack[colab,ocr]) (0.11.1+cu111)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.11,>1.9->farm-haystack[colab,ocr]) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->farm-haystack[colab,ocr]) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->farm-haystack[colab,ocr]) (2018.9)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx->farm-haystack[colab,ocr]) (4.2.6)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from quantulum3->farm-haystack[colab,ocr]) (2.1.0)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika->farm-haystack[colab,ocr]) (57.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting cryptography>=2.1.4\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm-haystack[colab,ocr]) (0.8.9)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b2->farm-haystack[colab,ocr]) (1.3.1)\n",
            "Collecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0->farm-haystack[colab,ocr]) (3.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow<=1.13.1->farm-haystack[colab,ocr]) (1.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow<=1.13.1->farm-haystack[colab,ocr]) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow<=1.13.1->farm-haystack[colab,ocr]) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow<=1.13.1->farm-haystack[colab,ocr]) (1.1.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words->quantulum3->farm-haystack[colab,ocr]) (0.6.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow<=1.13.1->farm-haystack[colab,ocr]) (0.13.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm-haystack[colab,ocr]) (1.15.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow<=1.13.1->farm-haystack[colab,ocr]) (2.0.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b2->farm-haystack[colab,ocr]) (3.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm-haystack[colab,ocr]) (2.21)\n",
            "Building wheels for collected packages: pytesseract, sentence-transformers, farm-haystack, langdetect, python-docx, seqeval, tika, alembic, databricks-cli\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.7-py2.py3-none-any.whl size=13954 sha256=da15c606884638036e4ba0163ffff50f294d77ad14cfcb854f736493f00140be\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/71/6c/7a8c5ca2e699752506999ae7baeb692e2b4fc6488c2cddcb22\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=120999 sha256=546ed35455a0be24fd44ceb9d0fb0db38dc0c2453da908f3c965b8a329c66442\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n",
            "  Building wheel for farm-haystack (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm-haystack: filename=farm_haystack-1.1.0-py3-none-any.whl size=405748 sha256=8d318cd274e0be542b9ad4ffd57c8ff5dc11318f60f6a851cac7c7b4dc0c976b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wv8eiyjr/wheels/a7/05/3b/9b33368d9af06a39f8e6af2e97fa2af876e893ade323cfc2c9\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=b746e90d44e640f51e59f52b0b1486f3292ebda66fddb1cde013d094d642da03\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=1d4357b88fbab977488a6e9d6239ab326aa4a36348b20d1eecef87b64762a005\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=81756a573213b4c7442cf146c95ce53f44f247d5509c7c6274b7e9d9090114c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32893 sha256=8bf20c7725b65176a78964400ad4303278a9d08621721796b472f6f7c86d99c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158171 sha256=6e39a1eb27e3b659af69621539358ac97db81d145640f0245c77f09d5c03f4e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.16.4-py3-none-any.whl size=106877 sha256=dc5b86e9c86c8ce2230dee564e2c1a8d0f727450d85decfaa634e2d4390700d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/a1/6d/fa1d22ea25ed8593887437fe1c7e00f6ef307fc240ccd4dc5c\n",
            "Successfully built pytesseract sentence-transformers farm-haystack langdetect python-docx seqeval tika alembic databricks-cli\n",
            "Installing collected packages: tokenizers, sentencepiece, python-editor, mmh3, azure-common, websocket-client, smmap, sacremoses, querystring-parser, pyyaml, python-docx, pytesseract, pydantic, pdf2image, num2words, Mako, langdetect, isodate, gunicorn, elasticsearch, elastic-apm, tika, quantulum3, huggingface-hub, gitdb, docker, databricks-cli, cryptography, azure-core, transformers, seqeval, prometheus-flask-exporter, msrest, gitpython, alembic, sentence-transformers, azure-storage-blob, azure-ai-formrecognizer, mlflow, farm-haystack\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed Mako-1.1.6 alembic-1.4.1 azure-ai-formrecognizer-3.2.0b2 azure-common-1.1.28 azure-core-1.22.0 azure-storage-blob-12.9.0 cryptography-36.0.1 databricks-cli-0.16.4 docker-5.0.3 elastic-apm-6.7.2 elasticsearch-7.10.0 farm-haystack-1.1.0 gitdb-4.0.9 gitpython-3.1.26 gunicorn-20.1.0 huggingface-hub-0.4.0 isodate-0.6.1 langdetect-1.0.9 mlflow-1.13.1 mmh3-3.0.0 msrest-0.6.21 num2words-0.5.10 pdf2image-1.14.0 prometheus-flask-exporter-0.18.7 pydantic-1.9.0 pytesseract-0.3.7 python-docx-0.8.11 python-editor-1.0.4 pyyaml-6.0 quantulum3-0.7.9 querystring-parser-1.2.4 sacremoses-0.0.47 sentence-transformers-2.1.0 sentencepiece-0.1.96 seqeval-1.2.2 smmap-5.0.0 tika-1.24 tokenizers-0.10.3 transformers-4.13.0 websocket-client-1.2.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m--2022-02-09 15:34:32--  https://dl.xpdfreader.com/xpdf-tools-linux-4.03.tar.gz\n",
            "Resolving dl.xpdfreader.com (dl.xpdfreader.com)... 45.79.72.155\n",
            "Connecting to dl.xpdfreader.com (dl.xpdfreader.com)|45.79.72.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23024720 (22M) [application/x-gzip]\n",
            "Saving to: ‘xpdf-tools-linux-4.03.tar.gz’\n",
            "\n",
            "xpdf-tools-linux-4. 100%[===================>]  21.96M  9.52MB/s    in 2.3s    \n",
            "\n",
            "2022-02-09 15:34:35 (9.52 MB/s) - ‘xpdf-tools-linux-4.03.tar.gz’ saved [23024720/23024720]\n",
            "\n",
            "xpdf-tools-linux-4.03/\n",
            "xpdf-tools-linux-4.03/ANNOUNCE\n",
            "xpdf-tools-linux-4.03/bin32/\n",
            "xpdf-tools-linux-4.03/bin32/pdftotext\n",
            "xpdf-tools-linux-4.03/bin32/pdfinfo\n",
            "xpdf-tools-linux-4.03/bin32/pdftopng\n",
            "xpdf-tools-linux-4.03/bin32/pdfimages\n",
            "xpdf-tools-linux-4.03/bin32/pdftoppm\n",
            "xpdf-tools-linux-4.03/bin32/pdftops\n",
            "xpdf-tools-linux-4.03/bin32/pdfdetach\n",
            "xpdf-tools-linux-4.03/bin32/pdffonts\n",
            "xpdf-tools-linux-4.03/bin32/pdftohtml\n",
            "xpdf-tools-linux-4.03/CHANGES\n",
            "xpdf-tools-linux-4.03/bin64/\n",
            "xpdf-tools-linux-4.03/bin64/pdftotext\n",
            "xpdf-tools-linux-4.03/bin64/pdfinfo\n",
            "xpdf-tools-linux-4.03/bin64/pdftopng\n",
            "xpdf-tools-linux-4.03/bin64/pdfimages\n",
            "xpdf-tools-linux-4.03/bin64/pdftoppm\n",
            "xpdf-tools-linux-4.03/bin64/pdftops\n",
            "xpdf-tools-linux-4.03/bin64/pdfdetach\n",
            "xpdf-tools-linux-4.03/bin64/pdffonts\n",
            "xpdf-tools-linux-4.03/bin64/pdftohtml\n",
            "xpdf-tools-linux-4.03/INSTALL\n",
            "xpdf-tools-linux-4.03/COPYING3\n",
            "xpdf-tools-linux-4.03/doc/\n",
            "xpdf-tools-linux-4.03/doc/pdftotext.1\n",
            "xpdf-tools-linux-4.03/doc/pdfdetach.1\n",
            "xpdf-tools-linux-4.03/doc/pdftopng.1\n",
            "xpdf-tools-linux-4.03/doc/sample-xpdfrc\n",
            "xpdf-tools-linux-4.03/doc/xpdfrc.5\n",
            "xpdf-tools-linux-4.03/doc/pdftops.1\n",
            "xpdf-tools-linux-4.03/doc/pdffonts.1\n",
            "xpdf-tools-linux-4.03/doc/pdftoppm.1\n",
            "xpdf-tools-linux-4.03/doc/pdfimages.1\n",
            "xpdf-tools-linux-4.03/doc/pdftohtml.1\n",
            "xpdf-tools-linux-4.03/doc/pdfinfo.1\n",
            "xpdf-tools-linux-4.03/COPYING\n",
            "xpdf-tools-linux-4.03/README\n"
          ]
        }
      ],
      "source": [
        "# Install the latest release of Haystack in your own environment\n",
        "#! pip install farm-haystack\n",
        "\n",
        "# Install the latest master of Haystack\n",
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab,ocr]\n",
        "\n",
        "!wget --no-check-certificate https://dl.xpdfreader.com/xpdf-tools-linux-4.03.tar.gz\n",
        "!tar -xvf xpdf-tools-linux-4.03.tar.gz && sudo cp xpdf-tools-linux-4.03/bin64/pdftotext /usr/local/bin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are the imports we need\n",
        "from haystack.nodes import (TextConverter, PDFToTextConverter,\n",
        "                            DocxToTextConverter, PreProcessor)\n",
        "from haystack.utils import convert_files_to_dicts, fetch_archive_from_http"
      ],
      "metadata": {
        "id": "wASt-AOrn_9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This fetches some sample files to work with\n",
        "\n",
        "doc_dir = 'data/preprocessing_tutorial'\n",
        "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/preprocessing_tutorial.zip\"\n",
        "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5qwTRKVtGMH",
        "outputId": "a8319113-bc80-468a-d305-9b9424481afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO - haystack.utils.import_utils -  Fetching from https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/preprocessing_tutorial.zip to `data/preprocessing_tutorial`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converters\n",
        "\n",
        "Haystack's converter classes are designed to help you turn files on your computer into the documents\n",
        "that can be processed by the Haystack pipeline.\n",
        "There are file converters for txt, pdf, docx files as well as a converter that is powered by Apache Tika.\n",
        "The parameter `valid_langugages` does not convert files to the target language, but checks if the conversion worked as expected.\n",
        "For converting PDFs, try changing the encoding to UTF-8 if the conversion isn't great."
      ],
      "metadata": {
        "id": "XFDgwFi1tQmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are some examples of how you would use file converters\n",
        "\n",
        "converter = TextConverter(remove_numeric_tables=True, \n",
        "                          valid_languages=['en'])\n",
        "doc_txt = converter.convert(file_path='data/preprocessing_tutorial/classics.txt',\n",
        "                            meta=None)[0]\n",
        "\n",
        "converter = PDFToTextConverter(remove_numeric_tables=True,\n",
        "                               valid_languages=['en'])\n",
        "doc_pdf = converter.convert(file_path='data/preprocessing_tutorial/bert.pdf',\n",
        "                            meta=None)[0]\n",
        "\n",
        "converter = DocxToTextConverter(remove_numeric_tables=False,\n",
        "                                valid_languages=['en'])\n",
        "doc_docx = converter.convert(file_path='data/preprocessing_tutorial/heavy_metal.docx',\n",
        "                             meta=None)[0]"
      ],
      "metadata": {
        "id": "-6oq4gH4tOyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Haystack also has a convenience function that will automatically apply\n",
        "# the right converter to each file in a directory\n",
        "\n",
        "all_docs = convert_files_to_dicts(dir_path='data/preprocessing_tutorial')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2teaYsdHifm",
        "outputId": "9313c35b-c0ea-470d-9c26-5983cd5bd40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO - haystack.utils.preprocessing -  Converting data/preprocessing_tutorial/heavy_metal.docx\n",
            "INFO - haystack.utils.preprocessing -  Converting data/preprocessing_tutorial/classics.txt\n",
            "INFO - haystack.utils.preprocessing -  Converting data/preprocessing_tutorial/bert.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PreProcessor\n",
        "\n",
        "The PreProcessor class is designed to help you clean text and split text into sensible units.\n",
        "File splitting can have a very significant impact on the system's performance and is absolutely mandatory for Dense Passage Retrieval models.\n",
        "In general, we recommend you split the text from your files into small documents of around 100 words for dense retrieval methods\n",
        "and no more than 10,000 words for sparse methods.\n",
        "Have a look at the [Preprocessing](https://haystack.deepset.ai/docs/latest/preprocessingmd)\n",
        "and [Optimization](https://haystack.deepset.ai/docs/latest/optimizationmd) pages on our website for more details."
      ],
      "metadata": {
        "id": "jRC1sK1EHy6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This a default usage of the PreProcessor\n",
        "# Here, it performs cleaning of consecutive whitespaces\n",
        "# and splits a single large document into smaller documents\n",
        "# Each document is up to 1000 words long and document breaks cannot fall in the\n",
        "# middle of sentences\n",
        "# Note how the single document passed into the document gets split into\n",
        "# 5 smaller documents\n",
        "\n",
        "preprocessor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=False,\n",
        "    split_by='word',\n",
        "    split_length=100,\n",
        "    split_respect_sentence_boundary=True,\n",
        ")\n",
        "\n",
        "docs_default = preprocessor.process(doc_txt)\n",
        "print(f'n_docs_input: 1\\nn_docs_output: {len(docs_default)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PvQ9H0FHzIp",
        "outputId": "fbfb6084-cc42-4d7c-f11d-f5a1f250342b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "n_docs_input: 1\n",
            "n_docs_output: 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning\n",
        "\n",
        "- `clean_empty_lines` will normalize 3 or more consecutive empty lines to be just a two empty lines\n",
        "- `clean_whitespace` will remove any whitespace at the beginning or end of each line in the text\n",
        "- `clean_header_footer` will remove any long header or footer texts that are repeated on each page"
      ],
      "metadata": {
        "id": "L658cDbKIqV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting\n",
        "By default, the PreProcessor will respect sentence boundaries, meaning that documents will not start or end\n",
        "midway through a sentence.\n",
        "This will help reduce the possibility of answer phrases being split between two documents.\n",
        "This feature can be turned off by setting `split_respect_sentence_boundary=False`."
      ],
      "metadata": {
        "id": "2XbF86HlIvqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Not respecting sentence boundary vs. respecting sentence boundary\n",
        "\n",
        "preprocessor_nrsb = PreProcessor(split_respect_sentence_boundary=False)\n",
        "docs_nrsb = preprocessor_nrsb.process(doc_txt)\n",
        "\n",
        "print('Respecting sentence boundary')\n",
        "end_text = docs_default[0]['content'][-50:]\n",
        "print('End of document: \"...' + end_text + '\"')\n",
        "print()\n",
        "print('Not respecting sentence boundary')\n",
        "end_text_nrsb = docs_nrsb[0]['content'][-50:]\n",
        "print('End of document: \"...' + end_text_nrsb + '\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KGj5B0aIqf5",
        "outputId": "a1a531ec-9beb-4a0a-b18f-4201351974ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respecting sentence boundary\n",
            "End of document: \"...cornerstone of a typical elite European education.\"\n",
            "\n",
            "Not respecting sentence boundary\n",
            "End of document: \"...xts used as part of a curriculum, both derive from\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A commonly used strategy to split long documents, especially in the field of Question Answering,\n",
        "is the sliding window approach. If `split_length=10` and `split_overlap=3`, your documents will look like this:\n",
        "\n",
        "- doc1 = words[0:10]\n",
        "- doc2 = words[7:17]\n",
        "- doc3 = words[14:24]\n",
        "- ...\n",
        "\n",
        "You can use this strategy by following the code below."
      ],
      "metadata": {
        "id": "58lfI4djMOWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sliding window approach\n",
        "\n",
        "preprocessor_sliding_window = PreProcessor(split_overlap=3, split_length=10,\n",
        "                                           split_respect_sentence_boundary=False)\n",
        "docs_sliding_window = preprocessor_sliding_window.process(doc_txt)\n",
        "\n",
        "doc1 = docs_sliding_window[0]['content'][:200]\n",
        "doc2 = docs_sliding_window[1]['content'][:100]\n",
        "doc3 = docs_sliding_window[2]['content'][:100]\n",
        "\n",
        "print('Document 1: \"' + doc1 + '...\"')\n",
        "print('Document 2: \"' + doc2 + '...\"')\n",
        "print('Document 3: \"' + doc3 + '...\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2-ljFN7MOmt",
        "outputId": "ddc721ec-866f-4e21-ce2a-7c84d71c272d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: \"Classics or classical studies is the study of classical antiquity,...\"\n",
            "Document 2: \"of classical antiquity, and in the Western world traditionally refers...\"\n",
            "Document 3: \"world traditionally refers to the study of Classical Greek and...\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bringing it all together"
      ],
      "metadata": {
        "id": "qy9WRohjM0ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_docs = convert_files_to_dicts(dir_path=\"data/preprocessing_tutorial\")\n",
        "preprocessor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=False,\n",
        "    split_by=\"word\",\n",
        "    split_length=100,\n",
        "    split_respect_sentence_boundary=True,\n",
        ")\n",
        "docs = preprocessor.process(all_docs)\n",
        "\n",
        "print(f\"n_files_input: {len(all_docs)}\\nn_docs_output: {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xtn84ECMyat",
        "outputId": "96ab1633-bdaa-4c0c-f429-60faea79afbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO - haystack.utils.preprocessing -  Converting data/preprocessing_tutorial/heavy_metal.docx\n",
            "INFO - haystack.utils.preprocessing -  Converting data/preprocessing_tutorial/classics.txt\n",
            "INFO - haystack.utils.preprocessing -  Converting data/preprocessing_tutorial/bert.pdf\n",
            "100%|██████████| 3/3 [00:00<00:00, 70.32docs/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_files_input: 3\n",
            "n_docs_output: 163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "roOb2whYM26c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}